{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aead1d5-c451-49ff-b38c-36db2464a4bf",
   "metadata": {},
   "source": [
    "# Entrenamiento de un Agente PPO con VizDoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f8599-8d25-4c38-988a-94488313faef",
   "metadata": {},
   "source": [
    "En este notebook, implementaremos un agente de Reinforcement Learning usando el algoritmo Proximal Policy Optimization (PPO) para jugar al juego de VizDoom.\n",
    "Trabajo realizado por:\n",
    "* Damián Pramparo\n",
    "* Federico Domínguez Gómez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3296c-6714-40cb-aebb-7fe4d6c24a30",
   "metadata": {},
   "source": [
    "## Importación de bibliotecas\n",
    "Usaremos las siguientes bibliotecas:\n",
    "* **vizdoom**: Librería para interactuar con el entorno de VizDoom.\n",
    "* **pandas**: Usada para crear un DataFrame y guardar datos en un archivo CSV.\n",
    "* **torch**: Biblioteca para el aprendizaje profundo con PyTorch.\n",
    "* **numpy**: Usada para manipular matrices numéricas.\n",
    "* **Pool**: Para poder entrenas varios agentes al mismo tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666dcdb-f3d3-4f8f-98c1-ea5fd29d57a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vizdoom\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8a862-0722-4476-a986-af6db3e0c425",
   "metadata": {},
   "source": [
    "# Definición de la clase PPOAgent\n",
    "Creamos una clase **PPOAgent** que representa nuestro agente PPO. Esta clase se encarga de definir la arquitectura de la política y de implementar los métodos necesarios para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65d1d2-e59f-4e5e-bbdc-46ab5060ddb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, alpha=0.99, epsilon=0.1):\n",
    "        self.policy_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, action_dim),\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.RMSprop(self.policy_net.parameters(), lr=learning_rate, alpha=alpha, eps=epsilon)\n",
    "        \n",
    "    def policy(self, state):\n",
    "        logits = self.policy_net(state)\n",
    "\n",
    "        logits = torch.squeeze(logits)\n",
    "\n",
    "        action_probs = torch.softmax(logits, dim=0)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        loss = self.ppo_loss(experiences)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def ppo_loss(self, experiences):\n",
    "        log_probs = [experience['log_probs'] for experience in experiences]\n",
    "        advantages = [experience['advantage'] for experience in experiences]\n",
    "        \n",
    "        log_probs_tensor = torch.tensor(log_probs, requires_grad=True)\n",
    "        advantages_tensor = torch.tensor(advantages, requires_grad=True)    \n",
    "\n",
    "        ## Agrego una dimension adicional sino da error \n",
    "        log_probs_tensor = log_probs_tensor.unsqueeze(1)  \n",
    "        advantages_tensor = advantages_tensor.unsqueeze(1) \n",
    "\n",
    "        policy_loss = -torch.mean(torch.sum(log_probs_tensor * advantages_tensor, dim=1))\n",
    "\n",
    "        return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6bfd96-aae1-4a8a-8839-f6e29bfbaeac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_actions = [\n",
    "    [1, 0, 0, 0, 0, 0, 0],  # MOVE_LEFT\n",
    "    [0, 1, 0, 0, 0, 0, 0],  # MOVE_RIGHT\n",
    "    [0, 0, 1, 0, 0, 0, 0],  # ATTACK\n",
    "    [0, 0, 0, 1, 0, 0, 0],  # MOVE_FORWARD\n",
    "    [0, 0, 0, 0, 1, 0, 0],  # MOVE_BACKWARD\n",
    "    [0, 0, 0, 0, 0, 1, 0],  # TURN_LEFT\n",
    "    [0, 0, 0, 0, 0, 0, 1],  # TURN_RIGHT\n",
    "]\n",
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"deadly_corridor.cfg\")\n",
    "game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "game.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c108a-96e5-448d-8b57-a5d8623e358f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_agent(agent_id, learning_rate, alpha, epsilon, sample_actions):\n",
    "    state_dim = 4\n",
    "    action_dim = 7\n",
    "    num_episodes = 100\n",
    "    \n",
    "    agent = PPOAgent(state_dim, action_dim, learning_rate, alpha, epsilon)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        game.new_episode()\n",
    "\n",
    "        state = torch.from_numpy(np.zeros((4,))).float() \n",
    "\n",
    "        experiences = []\n",
    "        while not game.is_episode_finished():\n",
    "            action = agent.policy(state)\n",
    "\n",
    "            game.make_action(sample_actions[action])\n",
    "\n",
    "            next_state = torch.from_numpy(np.zeros((4,))).float()\n",
    "            reward = game.get_last_reward()\n",
    "    \n",
    "            advantage = reward \n",
    "\n",
    "            log_prob = torch.log(agent.policy_net(state)[action])\n",
    "\n",
    "            experiences.append({\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'next_state': next_state,\n",
    "                'advantage': advantage,\n",
    "                'log_probs': log_prob\n",
    "            })\n",
    "        \n",
    "            state = next_state\n",
    "\n",
    "        agent.learn(experiences)\n",
    "        df = pd.DataFrame(experiences)\n",
    "        df.to_csv('datos.csv')\n",
    "        print('Episode {}: {}'.format(episode, game.get_total_reward()))\n",
    "\n",
    "    game.close()\n",
    "    return agent_id, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958021c-f66b-4c07-8abb-607d2d120bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Definimos las combinaciones de hiperparámetros a probar\n",
    "    hyperparameter_combinations = [\n",
    "        (0, 0.001, 0.99, 0.1),\n",
    "        (1, 0.001, 0.95, 0.2),\n",
    "        (2, 0.0005, 0.99, 0.05),\n",
    "        (3, 0.002, 0.98, 0.15)\n",
    "    ]\n",
    "    \n",
    "    # Creamos una piscina de procesos para entrenar a los agentes en paralelo\n",
    "    with Pool(4) as pool:\n",
    "        results = pool.starmap(train_agent, [(agent_id, lr, a, eps, sample_actions) for agent_id, lr, a, eps in hyperparameter_combinations])\n",
    "\n",
    "    # Procesamos los resultados\n",
    "    trained_agents = {agent_id: agent for agent_id, agent in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a928e0-f69e-40fc-9521-44ba7c0a7857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#no funciona, y no tengo idea por qué ni como hacer que ande\n",
    "# Definimos una función para evaluar a un agente en el entorno\n",
    "def evaluate_agent(agent, game, sample_actions, num_evaluation_episodes):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_evaluation_episodes):\n",
    "        game.new_episode()\n",
    "        state = torch.from_numpy(np.zeros((4,))).float()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            action = agent.policy(state)\n",
    "            game.make_action(sample_actions[action])\n",
    "            next_state = torch.from_numpy(np.zeros((4,))).float()\n",
    "            episode_reward += game.get_last_reward()\n",
    "            state = next_state\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    average_reward = np.mean(total_rewards)\n",
    "    return average_reward\n",
    "\n",
    "best_agent_id = None\n",
    "best_average_reward = float(\"-inf\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for agent_id, learning_rate, alpha, epsilon in hyperparameter_combinations:\n",
    "    print(f\"Evaluando Agente {agent_id} con hiperparámetros: lr={learning_rate}, alpha={alpha}, epsilon={epsilon}\")\n",
    "    \n",
    "    trained_agent = train_agent(agent_id, learning_rate, alpha, epsilon, sample_actions)\n",
    "    \n",
    "    average_reward = evaluate_agent(trained_agent, game, sample_actions, num_evaluation_episodes)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256ce15-d8bb-4b10-91ad-1054815a1f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd0289-253c-4a13-a454-4cce15f6f2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
