{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aead1d5-c451-49ff-b38c-36db2464a4bf",
   "metadata": {},
   "source": [
    "# Entrenamiento de un Agente PPO con VizDoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f8599-8d25-4c38-988a-94488313faef",
   "metadata": {},
   "source": [
    "En este notebook, implementaremos un agente de Reinforcement Learning usando el algoritmo Proximal Policy Optimization (PPO) para jugar al juego de VizDoom.\n",
    "Trabajo realizado por:\n",
    "* Damián Pramparo\n",
    "* Federico Domínguez Gómez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3296c-6714-40cb-aebb-7fe4d6c24a30",
   "metadata": {},
   "source": [
    "## Importación de bibliotecas\n",
    "Usaremos las siguientes bibliotecas:\n",
    "* **vizdoom**: Librería para interactuar con el entorno de VizDoom.\n",
    "* **pandas**: Usada para crear un DataFrame y guardar datos en un archivo CSV.\n",
    "* **torch**: Biblioteca para el aprendizaje profundo con PyTorch.\n",
    "* **numpy**: Usada para manipular matrices numéricas.\n",
    "* **Pool**: Para poder entrenas varios agentes al mismo tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a666dcdb-f3d3-4f8f-98c1-ea5fd29d57a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vizdoom\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8a862-0722-4476-a986-af6db3e0c425",
   "metadata": {},
   "source": [
    "# Definición de la clase PPOAgent\n",
    "Creamos una clase **PPOAgent** que representa nuestro agente PPO. Esta clase se encarga de definir la arquitectura de la política y de implementar los métodos necesarios para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d65d1d2-e59f-4e5e-bbdc-46ab5060ddb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, alpha=0.99, epsilon=0.1):\n",
    "        self.policy_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, action_dim),\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.RMSprop(self.policy_net.parameters(), lr=learning_rate, alpha=alpha, eps=epsilon)\n",
    "        self.total_reward = 0\n",
    "    def policy(self, state):\n",
    "        logits = self.policy_net(state)\n",
    "\n",
    "        logits = torch.squeeze(logits)\n",
    "\n",
    "        action_probs = torch.softmax(logits, dim=0)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        loss = self.ppo_loss(experiences)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def ppo_loss(self, experiences):\n",
    "        log_probs = [experience['log_probs'] for experience in experiences]\n",
    "        advantages = [experience['advantage'] for experience in experiences]\n",
    "        \n",
    "        log_probs_tensor = torch.tensor(log_probs, requires_grad=True)\n",
    "        advantages_tensor = torch.tensor(advantages, requires_grad=True)    \n",
    "\n",
    "        ## Agrego una dimension adicional sino da error \n",
    "        log_probs_tensor = log_probs_tensor.unsqueeze(1)  \n",
    "        advantages_tensor = advantages_tensor.unsqueeze(1) \n",
    "\n",
    "        policy_loss = -torch.mean(torch.sum(log_probs_tensor * advantages_tensor, dim=1))\n",
    "\n",
    "        return policy_loss\n",
    "    \n",
    "    def update_total_reward(self, reward):\n",
    "        self.total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6bfd96-aae1-4a8a-8839-f6e29bfbaeac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_actions = [\n",
    "    [1, 0, 0, 0, 0, 0, 0],  # MOVE_LEFT\n",
    "    [0, 1, 0, 0, 0, 0, 0],  # MOVE_RIGHT\n",
    "    [0, 0, 1, 0, 0, 0, 0],  # ATTACK\n",
    "    [0, 0, 0, 1, 0, 0, 0],  # MOVE_FORWARD\n",
    "    [0, 0, 0, 0, 1, 0, 0],  # MOVE_BACKWARD\n",
    "    [0, 0, 0, 0, 0, 1, 0],  # TURN_LEFT\n",
    "    [0, 0, 0, 0, 0, 0, 1],  # TURN_RIGHT\n",
    "]\n",
    "hyperparameter_combinations = [\n",
    "    (0, 0.001, 0.99, 0.1),\n",
    "    (1, 0.001, 0.95, 0.2),\n",
    "    (2, 0.0005, 0.99, 0.05),\n",
    "    (3, 0.002, 0.98, 0.15)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d7c108a-96e5-448d-8b57-a5d8623e358f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_agent(agent_id, learning_rate, alpha, epsilon, sample_actions):\n",
    "    state_dim = 4\n",
    "    action_dim = 7\n",
    "    num_episodes = 1000\n",
    "    \n",
    "    agent = PPOAgent(state_dim, action_dim, learning_rate, alpha, epsilon)\n",
    "    \n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config(\"deadly_corridor.cfg\")\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        total_reward = 0\n",
    "        game.new_episode()\n",
    "\n",
    "        state = torch.from_numpy(np.zeros((4,))).float() \n",
    "\n",
    "        experiences = []\n",
    "        while not game.is_episode_finished():\n",
    "            action = agent.policy(state)\n",
    "\n",
    "            game.make_action(sample_actions[action])\n",
    "\n",
    "            next_state = torch.from_numpy(np.zeros((4,))).float()\n",
    "            reward = game.get_last_reward()\n",
    "    \n",
    "            advantage = reward \n",
    "\n",
    "            log_prob = torch.log(agent.policy_net(state)[action])\n",
    "\n",
    "            experiences.append({\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'next_state': next_state,\n",
    "                'advantage': advantage,\n",
    "                'log_probs': log_prob\n",
    "            })\n",
    "        \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        agent.update_total_reward(total_reward)\n",
    "        agent.learn(experiences)\n",
    "        df = pd.DataFrame(experiences)\n",
    "        df.to_csv('datos.csv')\n",
    "#        print('Episode {}: {}'.format(episode, game.get_total_reward()))\n",
    "\n",
    "    game.close()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d20412-aebe-44e4-b583-e46a9e309c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rl_values, rewards):\n",
    "    \"\"\"\n",
    "    Genera una gráfica de recompensas en función de los valores de RL.\n",
    "\n",
    "    :param rl_values: Lista de valores de resistencia RL.\n",
    "    :param rewards: Lista de recompensas correspondientes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rl_values, rewards, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Recompensas vs. Valores de RL')\n",
    "    plt.xlabel('Valor de RL')\n",
    "    plt.ylabel('Recompensas')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958021c-f66b-4c07-8abb-607d2d120bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_agents = {}\n",
    "rewards_by_hyperparameter = {} \n",
    "\n",
    "for agent_id, lr, a, eps in hyperparameter_combinations:\n",
    "    agent = train_agent(agent_id, lr, a, eps, sample_actions)\n",
    "    trained_agents[agent_id] = agent\n",
    "\n",
    "# Recolectar recompensas para cada combinación de hiperparámetros\n",
    "for agent_id, agent in trained_agents.items():\n",
    "    rewards = agent.total_reward\n",
    "    rewards_by_hyperparameter[agent_id] = rewards\n",
    "\n",
    "rl_labels = [f\"Comb {params[0]}: LR={params[1]}, A={params[2]}, EPS={params[3]}\" for params in hyperparameter_combinations]\n",
    "rewards = [rewards_by_hyperparameter[agent_id] for agent_id in rewards_by_hyperparameter]\n",
    "\n",
    "\n",
    "plot_rewards(rl_labels, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd0289-253c-4a13-a454-4cce15f6f2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
